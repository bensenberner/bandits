% Encoding: ISO-8859-1


@InCollection{ic-Abbasi-Yadkori2011,
  Title                    = {{Improved Algorithms for Linear Stochastic Bandits}},
  Author                   = {Yasin Abbasi-Yadkori and P\'{a}l, D\'{a}vid and Csaba Szepesv\'{a}ri},
  Booktitle                = {Advances in Neural Information Processing Systems 24},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2011},
  Editor                   = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
  Pages                    = {2312--2320},

  Owner                    = {iurteaga},
  Timestamp                = {2018.07.30},
  Url                      = {https://papers.nips.cc/paper/4417-improved-algorithms-for-linear-stochastic-bandits}
}

@Article{j-Agrawal2012,
  Title                    = {{Thompson Sampling for Contextual Bandits with Linear Payoffs}},
  Author                   = {Shipra Agrawal and Navin Goyal},
  Journal                  = {CoRR},
  Year                     = {2012},
  Volume                   = {abs/1209.3352},

  Owner                    = {iurteaga},
  Timestamp                = {2017.05.10}
}

@Article{j-Agrawal2012a,
  Title                    = {{Further Optimal Regret Bounds for Thompson Sampling}},
  Author                   = {Shipra Agrawal and Navin Goyal},
  Journal                  = {CoRR},
  Year                     = {2012},
  Volume                   = {abs/1209.3353},

  Owner                    = {iurteaga},
  Timestamp                = {2017.05.10}
}

@Article{j-Agrawal2011,
  Title                    = {{Analysis of Thompson Sampling for the multi-armed bandit problem}},
  Author                   = {Shipra Agrawal and Navin Goyal},
  Journal                  = {CoRR},
  Year                     = {2011},
  Volume                   = {abs/1111.1797},

  Abstract                 = {The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems. Many algorithms are now available for this well-studied problem. One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm, referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose an arm to play according to its probability of being the best arm. Thompson Sampling algorithm has experimentally been shown to be close to optimal. In addition, it is efficient to implement and exhibits several desirable properties such as small regret for delayed feedback. However, theoretical understanding of this algorithm was quite limited. In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the multi-armed bandit problem. More precisely, for the two-armed bandit problem, the expected regret in time T is O(lnTΔ+1Δ3). And, for the N-armed bandit problem, the expected regret in time T is O([(∑Ni=21Δ2i)2]lnT). Our bounds are optimal but for the dependence on Δi and the constant factors in big-Oh.},
  Keywords                 = {Thompson sampling, multi-armed bandit, bounds},
  Owner                    = {iurteaga},
  Timestamp                = {2016.10.20}
}

@Article{j-Arulampalam2002,
  Title                    = {{A tutorial on particle filters for online nonlinear/non-{G}aussian {B}ayesian tracking}},
  Author                   = {M. Sanjeev Arulampalam and Simon Maskell and Neil Gordon and Tim Clapp},
  Journal                  = {Signal Processing, IEEE Transactions on},
  Year                     = {2002},

  Month                    = {2},
  Number                   = {2},
  Pages                    = {174-188},
  Volume                   = {50},

  Abstract                 = {Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or "particle") representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example},
  ISSN                     = {1053-587X},
  Keywords                 = {Bayes methods;Kalman filters;Monte Carlo methods;filtering theory;importance sampling;state estimation;state-space methods;tracking filters;Kalman filtering;nonGaussian tracking problems;nonlinear tracking problems;optimal Bayesian algorithms;particle filters;point mass representations;probability densities;sequential Monte Carlo methods;sequential importance sampling;state-space model;suboptimal Bayesian algorithms;tutorial;Bayesian methods;Costs;Filtering;Kalman filters;Monte Carlo methods;Nonlinear dynamical systems;Particle filters;Particle tracking;Signal processing;Tutorial},
  Owner                    = {iurteaga},
  Timestamp                = {2015-11-03}
}

@Article{j-Auer2002,
  Title                    = {{Finite-time Analysis of the Multiarmed Bandit Problem}},
  Author                   = {Peter Auer and Nicol\`{o} Cesa-Bianchi and Paul Fischer},
  Journal                  = {Machine Learning},
  Year                     = {2002},

  Month                    = may,
  Number                   = {2-3},
  Pages                    = {235--256},
  Volume                   = {47},

  Acmid                    = {599677},
  Address                  = {Hingham, MA, USA},
  Doi                      = {10.1023/A:1013689704352},
  ISSN                     = {0885-6125},
  Issue_date               = {May-June 2002},
  Keywords                 = {adaptive allocation rules, bandit problems, finite horizon regret},
  Numpages                 = {22},
  Owner                    = {iurteaga},
  Publisher                = {Kluwer Academic Publishers},
  Timestamp                = {2017.05.10}
}

@Article{j-Bellm1956,
  Title                    = {{A Problem in the Sequential Design of Experiments}},
  Author                   = {Richard Bellman},
  Journal                  = {Sankhya: The Indian Journal of Statistics (1933 - 1960)},
  Year                     = {1956},
  Number                   = {3/4},
  Pages                    = {221-229},
  Volume                   = {16},

  Owner                    = {iurteaga},
  Publisher                = {Springer},
  Timestamp                = {2017.05.10}
}

@Book{b-Bernardo2009,
  Title                    = {{Bayesian Theory}},
  Author                   = {Jos\'{e} M. Bernardo and Adrian F.M. Smith},
  Publisher                = {Wiley},
  Year                     = {2009},
  Series                   = {Wiley Series in Probability and Statistics},

  Doi                      = {10.1002/9780470316870},
  ISBN                     = {9780470317716},
  Owner                    = {iurteaga},
  Timestamp                = {2017.05.10}
}

@Article{j-Beygelzimer2008,
  Title                    = {{The Offset Tree for Learning with Partial Labels}},
  Author                   = {Alina Beygelzimer and John Langford},
  Journal                  = {CoRR},
  Year                     = {2008},
  Volume                   = {abs/0812.4044},

  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@Book{b-Bishop2006,
  Title                    = {{Pattern Recognition and Machine Learning}},
  Author                   = {Christopher Bishop},
  Publisher                = {Springer-Verlag New York},
  Year                     = {2006},
  Series                   = {Information Science and Statistics},

  Owner                    = {iurteaga},
  Timestamp                = {2017.05.13}
}

@Article{j-Blei2012,
  Title                    = {{Probabilistic Topic Models}},
  Author                   = {David M. Blei},
  Journal                  = {Commun. ACM},
  Year                     = {2012},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {77--84},
  Volume                   = {55},

  Acmid                    = {2133826},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/2133806.2133826},
  ISSN                     = {0001-0782},
  Issue_date               = {April 2012},
  Numpages                 = {8},
  Owner                    = {iurteaga},
  Publisher                = {ACM},
  Timestamp                = {2016.11.15}
}

@Article{j-Blei2006a,
  Title                    = {{Variational inference for Dirichlet process mixtures}},
  Author                   = {David M. Blei and Michael I. Jordan},
  Journal                  = {Bayesian analysis},
  Year                     = {2006},
  Number                   = {1},
  Pages                    = {121--144},
  Volume                   = {1},

  Abstract                 = {Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled the application of nonparametric Bayesian methods to a variety of practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it is important to explore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for DP mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@InProceedings{ip-Blei2004,
  Title                    = {{Variational Methods for the Dirichlet Process}},
  Author                   = {David M. Blei and Michael I. Jordan},
  Booktitle                = {Proceedings of the Twenty-first International Conference on Machine Learning},
  Year                     = {2004},

  Address                  = {New York, NY, USA},
  Pages                    = {12--},
  Publisher                = {ACM},
  Series                   = {ICML '04},

  Acmid                    = {1015439},
  Doi                      = {10.1145/1015330.1015439},
  ISBN                     = {1-58113-838-5},
  Location                 = {Banff, Alberta, Canada},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@InProceedings{ip-Blei2006,
  Title                    = {{Dynamic topic models}},
  Author                   = {David M. Blei and John D. Lafferty},
  Booktitle                = {Proceedings of the 23rd international conference on Machine learning},
  Year                     = {2006},
  Organization             = {ACM},
  Pages                    = {113--120},

  Abstract                 = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR’ed archives of the journal Science from 1880 through 2000.},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@Article{j-Blei2006,
  Title                    = {{Correlated topic models}},
  Author                   = {David M. Blei and John D. Lafferty},
  Journal                  = {Advances in neural information processing systems},
  Year                     = {2006},
  Pages                    = {147},
  Volume                   = {18},

  Owner                    = {iurteaga},
  Publisher                = {MIT; 1998},
  Timestamp                = {2016.11.15}
}

@Article{j-Blei2003,
  Title                    = {{Latent Dirichlet allocation}},
  Author                   = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  Journal                  = {Journal of machine Learning research},
  Year                     = {2003},
  Number                   = {Jan},
  Pages                    = {993--1022},
  Volume                   = {3},

  Abstract                 = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@InProceedings{ip-Blundell2015,
  Title                    = {{Weight Uncertainty in Neural Networks}},
  Author                   = {Charles Blundell and Julien Cornebise and Koray Kavukcuoglu and Daan Wierstra},
  Booktitle                = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
  Year                     = {2015},
  Pages                    = {1613--1622},
  Publisher                = {JMLR.org},
  Series                   = {ICML'15},

  Acmid                    = {3045290},
  Location                 = {Lille, France},
  Numpages                 = {10},
  Owner                    = {iurteaga},
  Timestamp                = {2017.11.28}
}

@Article{j-Bottou2012,
  Title                    = {{Counterfactual Reasoning and Learning Systems}},
  Author                   = {L{\'{e}}on Bottou and Jonas Peters and Joaquin Qui{\~{n}}onero Candela and Denis Xavier Charles and Max Chickering and Elon Portugaly and Dipankar Ray and Patrice Y. Simard and Ed Snelson},
  Journal                  = {CoRR},
  Year                     = {2012},
  Volume                   = {abs/1209.2355},

  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@Article{j-Brezzi2002,
  Title                    = {{Optimal learning and experimentation in bandit problems}},
  Author                   = {Monica Brezzi and Tze Leung Lai},
  Journal                  = {Journal of Economic Dynamics and Control},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {87 - 108},
  Volume                   = {27},

  Doi                      = {https://doi.org/10.1016/S0165-1889(01)00028-8},
  ISSN                     = {0165-1889},
  Owner                    = {iurteaga},
  Timestamp                = {2017.05.10}
}

@Article{j-Brezzi2000,
  Title                    = {{Incomplete Learning from Endogenous Data in Dynamic Allocation}},
  Author                   = {Monica Brezzi and Tze Leung Lai},
  Journal                  = {Econometrica},
  Year                     = {2000},
  Number                   = {6},
  Pages                    = {1511--1516},
  Volume                   = {68},

  Doi                      = {10.1111/1468-0262.00170},
  ISSN                     = {1468 0262},
  Owner                    = {iurteaga},
  Publisher                = {Blackwell Publishers Ltd},
  Timestamp                = {2017.05.10}
}

@Article{j-Burnetas1997,
  Title                    = {{Optimal Adaptive Policies for Markov Decision Processes}},
  Author                   = {Apostolos N. Burnetas and Michael N. Katehakis},
  Journal                  = {Mathematics of Operations Research},
  Year                     = {1997},
  Number                   = {1},
  Pages                    = {222-255},
  Volume                   = {22},

  Abstract                 = { In this paper we consider the problem of adaptive control for Markov Decision Processes. We give the explicit form for a class of adaptive policies that possess optimal increase rate properties for the total expected finite horizon reward, under sufficient assumptions of finite state-action spaces and irreducibility of the transition law. A main feature of the proposed policies is that the choice of actions, at each state and time period, is based on indices that are inflations of the right-hand side of the estimated average reward optimality equations.},
  Doi                      = {10.1287/moor.22.1.222},
  Owner                    = {iurteaga},
  Timestamp                = {2018.07.30}
}

@Article{j-Carvalho2010,
  Title                    = {{Particle Learning and Smoothing}},
  Author                   = {Carlos M Carvalho and Michael S. Johannes and Hedibert F. Lopes and Nicholas G. Polson},
  Journal                  = {Statist. Sci.},
  Year                     = {2010},

  Month                    = {02},
  Number                   = {1},
  Pages                    = {88--106},
  Volume                   = {25},

  Fjournal                 = {Statistical Science},
  Owner                    = {iurteaga},
  Publisher                = {The Institute of Mathematical Statistics},
  Timestamp                = {2017-09-30}
}

@Article{j-Cesa-Bianchi2011,
  Title                    = {{An Optimal Algorithm for Linear Bandits}},
  Author                   = {Nicol\`o Cesa-Bianchi and Sham Kakade},
  Journal                  = {ArXiv e-prints},
  Year                     = {2011},

  Month                    = oct,

  Archiveprefix            = {arXiv},
  Eprint                   = {1110.4322},
  Keywords                 = {Computer Science - Learning, Statistics - Machine Learning},
  Owner                    = {iurteaga},
  Primaryclass             = {cs.LG},
  Timestamp                = {2018.07.30}
}

@InProceedings{ip-Chang2009,
  Title                    = {{Reading Tea Leaves: How Humans Interpret Topic Models}},
  Author                   = {Jonathan Chang and Jordan Boyd-Graber and Chong Wang and Sean Gerrish and David M. Blei},
  Booktitle                = {Neural Information Processing Systems},
  Year                     = {2009},

  Keywords                 = {Topic models, interpretation, held-out likelihood},
  Location                 = {Vancouver, BC},
  Owner                    = {iurteaga},
  Timestamp                = {2017.04.20}
}

@InCollection{ic-Chapelle2011,
  Title                    = {{An Empirical Evaluation of Thompson Sampling}},
  Author                   = {Olivier Chapelle and Lihong Li},
  Booktitle                = {Advances in Neural Information Processing Systems 24},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2011},
  Editor                   = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
  Pages                    = {2249--2257},

  Owner                    = {iurteaga},
  Timestamp                = {2016.09.19}
}

@Article{j-Chopin2004,
  Title                    = {{Central Limit Theorem for Sequential Monte Carlo Methods and Its Application to Bayesian Inference}},
  Author                   = {Nicolas Chopin},
  Journal                  = {The Annals of Statistics},
  Year                     = {2004},
  Number                   = {6},
  Pages                    = {2385-2411},
  Volume                   = {32},

  ISSN                     = {00905364},
  Owner                    = {iurteaga},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2018.02.06}
}

@Article{j-Chopin2011,
  Title                    = {{SMC\^{2}: an efficient algorithm for sequential analysis of state-space models}},
  Author                   = {Nicolas Chopin and Pierre E. Jacob and Omiros Papaspiliopoulos},
  Journal                  = {ArXiv e-prints},
  Year                     = {2011},

  Month                    = jan,

  Archiveprefix            = {arXiv},
  Eprint                   = {1101.1528},
  Keywords                 = {Statistics - Computation, 62F15, 65C05},
  Owner                    = {iurteaga},
  Primaryclass             = {stat.CO},
  Timestamp                = {2017-09-30}
}

@InProceedings{ip-Chu2011,
  Title                    = {{Contextual Bandits with Linear Payoff Functions}},
  Author                   = {Wei Chu and Lihong Li and Lev Reyzin and Robert Schapire},
  Booktitle                = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  Year                     = {2011},

  Address                  = {Fort Lauderdale, FL, USA},
  Editor                   = {Geoffrey Gordon and David Dunson and Miroslav Dud\'ik},
  Month                    = {11--13 Apr},
  Pages                    = {208--214},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {15},

  Abstract                 = {In this paper we study the contextual bandit problem (also known as the multi-armed bandit problem with expert advice) for linear payoff functions. For T rounds, K actions, and d dimensional feature vectors, we prove an $O(\sqrt{Td\ln^3(KT\ln(T)/\delta}))$ regret bound that holds with probability $1-\delta$ for the simplest known (both conceptually and computationally) efficient upper confidence bound algorithm for this problem. We also prove a lower bound of \Omega(\sqrt{Td})$ for this setting, matching the upper bound up to logarithmic factors.},
  Url                      = {http://proceedings.mlr.press/v15/chu11a.html}
}

@InProceedings{ip-Chu2009,
  Title                    = {{A Case Study of Behavior-driven Conjoint Analysis on Yahoo!: Front Page Today Module}},
  Author                   = {Chu, Wei and Park, Seung-Taek and Beaupre, Todd and Motgi, Nitin and Phadke, Amit and Chakraborty, Seinjuti and Zachariah, Joe},
  Booktitle                = {Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  Year                     = {2009},

  Address                  = {New York, NY, USA},
  Pages                    = {1097--1104},
  Publisher                = {ACM},
  Series                   = {KDD '09},

  Acmid                    = {1557138},
  Doi                      = {10.1145/1557019.1557138},
  ISBN                     = {978-1-60558-495-9},
  Keywords                 = {classification, clustering, conjoint analysis, logistic regression, segmentation, tensor product},
  Location                 = {Paris, France},
  Numpages                 = {8},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.15}
}

@Article{j-Creal2012,
  Title                    = {{A Survey of Sequential Monte Carlo Methods for Economics and Finance}},
  Author                   = {Drew Creal},
  Journal                  = {Econometric Reviews},
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {245-296},
  Volume                   = {31},

  Abstract                 = { This article serves as an introduction and survey for economists to the field of sequential Monte Carlo methods which are also known as particle filters. Sequential Monte Carlo methods are simulation-based algorithms used to compute the high-dimensional and/or complex integrals that arise regularly in applied work. These methods are becoming increasingly popular in economics and finance; from dynamic stochastic general equilibrium models in macro-economics to option pricing. The objective of this article is to explain the basics of the methodology, provide references to the literature, and cover some of the theoretical results that justify the methods in practice.},
  Keywords                 = {Kalman filter, Markov chain Monte Carlo, Particle filter, Sequential Monte Carlo, State space models, Finance, Economics},
  Owner                    = {iurteaga},
  Timestamp                = {2013-11-14}
}

@Article{j-Crisan2002,
  Title                    = {{A survey of convergence results on particle filtering methods for practitioners}},
  Author                   = {Dan Crisan and Arnaud Doucet},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {2002},

  Month                    = {Mar},
  Number                   = {3},
  Pages                    = {736-746},
  Volume                   = {50},

  Abstract                 = {Optimal filtering problems are ubiquitous in signal processing and related fields. Except for a restricted class of models, the optimal filter does not admit a closed-form expression. Particle filtering methods are a set of flexible and powerful sequential Monte Carlo methods designed to. solve the optimal filtering problem numerically. The posterior distribution of the state is approximated by a large set of Dirac-delta masses (samples/particles) that evolve randomly in time according to the dynamics of the model and the observations. The particles are interacting; thus, classical limit theorems relying on statistically independent samples do not apply. In this paper, our aim is to present a survey of convergence results on this class of methods to make them accessible to practitioners},
  Doi                      = {10.1109/78.984773},
  ISSN                     = {1053-587X},
  Keywords                 = {Monte Carlo methods;convergence of numerical methods;filtering theory;optimisation;signal processing;Dirac-delta masses;closed-form expression;convergence results survey;general state-space models;model dynamics;optimal filter;optimal filtering;particle filtering methods;posterior distribution;recursive algorithm;samples/particles;sequential Monte Carlo methods;signal processing;statistically independent samples;Bayesian methods;Closed-form solution;Convergence;Design methodology;Filtering algorithms;Filters;Hidden Markov models;Monte Carlo methods;Signal processing;State estimation},
  Owner                    = {iurteaga},
  Timestamp                = {2018.02.06}
}

@Article{j-Crisan2013,
  Title                    = {{Nested particle filters for online parameter estimation in discrete-time state-space Markov models}},
  Author                   = {Dan Crisan and Joaqu\'{i}n M\'{i}guez},
  Journal                  = {ArXiv e-prints},
  Year                     = {2013},

  Month                    = {Aug},

  Archiveprefix            = {arXiv},
  Eprint                   = {1308.1883},
  Keywords                 = {Statistics - Computation, Mathematics - Numerical Analysis, Mathematics - Probability, 60J05, 60F05, 65C05},
  Owner                    = {iurteaga},
  Primaryclass             = {stat.CO},
  Timestamp                = {2017-09-30}
}

@Article{j-Dempster1977,
  Title                    = {{Maximum likelihood from incomplete data via the EM algorithm}},
  Author                   = {Arthur P. Dempster and Nan M. Laird and Donald B. Rubin},
  Journal                  = {Journal of the royal statistical society. Series B (methodological)},
  Year                     = {1977},
  Pages                    = {1--38},

  Owner                    = {iurteaga},
  Publisher                = {Royal Statistical Society},
  Timestamp                = {2017.05.19}
}

@InBook{ib-Djuric2010,
  Title                    = {{Particle Filtering}},
  Author                   = {Petar M. Djuri\'{c} and M\'{o}nica F. Bugallo},
  Chapter                  = {5},
  Pages                    = {271-331},
  Publisher                = {Wiley-Blackwell},
  Year                     = {2010},

  Abstract                 = {Summary This chapter contains sections titled: Introduction Motivation for Use of Particle Filtering The Basic Idea The Choice of Proposal Distribution and Resampling Some Particle Filtering Methods Handling Constant Parameters Rao Blackwellization Prediction Smoothing Convergence Issues Computational Issues and Hardware Implementation Acknowledgments Exercises References},
  Booktitle                = {Adaptive Signal Processing},
  Doi                      = {10.1002/9780470575758.ch5},
  Eprint                   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470575758.ch5},
  ISBN                     = {9780470575758},
  Keywords                 = {motivation for use of particle filtering, choice of proposal distribution and resampling, Rao Blackwellization, variance of estimate reduction, and Monte Carlo sampling methods},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.15}
}

@InProceedings{ip-Djuric2004,
  Title                    = {{Density assisted particle filters for state and parameter estimation}},
  Author                   = {Petar M. Djuri\'{c} and M\'{o}nica F. Bugallo and Joaqu\'{i}n M\'{i}guez},
  Booktitle                = {2004 IEEE International Conference on Acoustics, Speech, and Signal Processing, (ICASSP)},
  Year                     = {2004},
  Month                    = {5},
  Pages                    = {ii - 701-704},
  Volume                   = {2},

  Abstract                 = {In recent years the theory of particle filtering has continued to advance, and it has found increasing use in sequential signal processing. A weakness of particle filtering is that it is inadequate for problems that besides tracking of evolving states require the estimation of constant parameters. In this paper, we propose particle filters that do not have this limitation. We call these filters density assisted particle filters, of which special cases are the recently introduced Gaussian particle filters and Gaussian sum particle filters. An implementation of a density particle filter is shown on a relatively simple but important nonlinear model. Simulations are included that show the performance of this filter.},
  Doi                      = {10.1109/ICASSP.2004.1326354},
  ISSN                     = {1520-6149},
  Keywords                 = {Gaussian particle filters; Gaussian sum particle filters; density assisted particle filters; nonlinear model; parameter estimation; particle filtering; performance; sequential signal processing; state estimation; Gaussian distribution; filtering theory; nonlinear filters; parameter estimation; sequential estimation; signal sampling; state estimation;},
  Owner                    = {iurteaga},
  Timestamp                = {2012-08-29}
}

@Article{j-Djuric2003,
  Title                    = {{Particle Filtering}},
  Author                   = {Petar M. Djuri\'{c} and Jayesh H. Kotecha and Jianqui Zhang and Yufei Huang and Tadesse Ghirmai and M\'{o}nica F. Bugallo and Joaqu\'{i}n M\'{i}guez},
  Journal                  = {IEEE Signal Processing Magazine},
  Year                     = {2003},

  Month                    = {9},
  Pages                    = {19-38},
  Volume                   = {20(5)},

  Abstract                 = {Recent developments have demonstrated that particle filtering is an emerging and powerful methodology for sequential signal processing with a wide range of applications in science and engineering. It has captured the attention of many researchers in various communities including those of signal processing, statistics, and econometrics, and this interest stems from its potential for coping with difficult nonlinear and/or non-Gaussian problems. Based on the concept of sequential importance sampling and the use of Bayesian theory, particle filtering is particularly useful in dealing with nonlinear and non-Gaussian problems. The underlying principle of the methodology is the approximation of relevant distributions with random measures composed of particles (samples from the space of the unknowns) and their associated weights. In this article, first we present a brief review of the particle filtering theory, and then we show how it can be used for resolving many problems in wireless communications. We demonstrate its application to blind equalization, blind detection over flat fading channels, multiuser detection, and estimation and detection of space-time codes in fading channels.},
  Keywords                 = {Particle Filtering, review, wireleless communications},
  Owner                    = {iurteaga},
  Timestamp                = {2012-05-31}
}

@InProceedings{ip-Doucet2000,
  Title                    = {{Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks}},
  Author                   = {Arnaud Doucet and Nando de Freitas and Kevin P. Murphy and Stuart J. Russell},
  Booktitle                = {Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence},
  Year                     = {2000},

  Address                  = {San Francisco, CA, USA},
  Pages                    = {176--183},
  Publisher                = {Morgan Kaufmann Publishers Inc.},
  Series                   = {UAI '00},

  Acmid                    = {720075},
  ISBN                     = {1-55860-709-9},
  Numpages                 = {8},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.15}
}

@Article{j-Gershman2012,
  Title                    = {{A tutorial on Bayesian nonparametric models}},
  Author                   = {Samuel J. Gershman and David M. Blei},
  Journal                  = {Journal of Mathematical Psychology},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {1 - 12},
  Volume                   = {56},

  Doi                      = {https://doi.org/10.1016/j.jmp.2011.08.004},
  ISSN                     = {0022-2496},
  Keywords                 = {Bayesian methods, Chinese restaurant process, Indian buffet process},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@Article{j-Ghavamzadeh2015,
  Title                    = {{Bayesian Reinforcement Learning: A Survey}},
  Author                   = {Mohammad Ghavamzadeh and Shie Mannor and Joelle Pineau and Aviv Tamar},
  Journal                  = {Foundations and Trends® in Machine Learning},
  Year                     = {2015},
  Number                   = {5-6},
  Pages                    = {359-483},
  Volume                   = {8},

  Abstract                 = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/ exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.},
  Doi                      = {10.1561/2200000049},
  ISSN                     = {1935-8237},
  Keywords                 = {Reinforcement Learning, Bayesian},
  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@Article{j-Gittins1979,
  Title                    = {{Bandit Processes and Dynamic Allocation Indices}},
  Author                   = {J. C. Gittins},
  Journal                  = {Journal of the Royal Statistical Society. Series B (Methodological)},
  Year                     = {1979},
  Number                   = {2},
  Pages                    = {148-177},
  Volume                   = {41},

  Abstract                 = {The paper aims to give a unified account of the central concepts in recent work on bandit processes and dynamic allocation indices; to show how these reduce some previously intractable problems to the problem of calculating such indices; and to describe how these calculations may be carried out. Applications to stochastic scheduling, sequential clinical trials and a class of search problems are discussed.},
  ISSN                     = {00359246},
  Owner                    = {iurteaga},
  Publisher                = {[Royal Statistical Society, Wiley]},
  Timestamp                = {2016.09.20}
}

@InProceedings{ip-Gopalan2015,
  Title                    = {{Thompson Sampling for Learning Parameterized Markov Decision Processes}},
  Author                   = {Aditya Gopalan and Shie Mannor},
  Booktitle                = {Proceedings of The 28th Conference on Learning Theory},
  Year                     = {2015},

  Address                  = {Paris, France},
  Editor                   = {Peter Gr\"unwald and Elad Hazan and Satyen Kale},
  Month                    = {03--06 Jul},
  Pages                    = {861--898},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {40},

  Abstract                 = {We consider reinforcement learning in parameterized Markov Decision Processes (MDPs), where the parameterization may induce correlation across transition probabilities or rewards. Consequently, observing a particular state transition might yield useful information about other, unobserved, parts of the MDP. We present a version of Thompson sampling for parameterized reinforcement learning problems, and derive a frequentist regret bound for priors over general parameter spaces. The result shows that the number of instants where suboptimal actions are chosen scales logarithmically with time, with high probability. It holds for prior distributions that put significant probability near the true model, without any additional, specific closed-form structure such as conjugate or product-form priors. The constant factor in the logarithmic scaling encodes the information complexity of learning the MDP in terms of the Kullback-Leibler geometry of the parameter space.},
  File                     = {Gopalan15.pdf:http\://proceedings.mlr.press/v40/Gopalan15.pdf:PDF},
  Url                      = {http://proceedings.mlr.press/v40/Gopalan15.html}
}

@Article{j-Gordon1993,
  Title                    = {{Novel approach to nonlinear/non-Gaussian Bayesian state estimation}},
  Author                   = {Neil J. Gordon and D.J. Salmond and A.F.M. Smith},
  Journal                  = {Radar and Signal Processing, IEEE Proceedings},
  Year                     = {1993},

  Month                    = {4},
  Number                   = {2},
  Pages                    = {107 -113},
  Volume                   = {140},

  Abstract                 = {An algorithm, the bootstrap filter, is proposed for implementing recursive Bayesian filters. The required density of the state vector is represented as a set of random samples, which are updated and propagated by the algorithm. The method is not restricted by assumptions of linearity or Gaussian noise: it may be applied to any state transition or measurement model. A simulation example of the bearings only tracking problem is presented. This simulation includes schemes for improving the efficiency of the basic algorithm. For this example, the performance of the bootstrap filter is greatly superior to the standard extended Kalman filter},
  ISSN                     = {0956-375X},
  Keywords                 = {Gaussian noise;algorithm;bearings only tracking problem;bootstrap filter;extended Kalman filter;measurement model;nonGaussian Bayesian state estimation;nonlinear Bayesian state estimation;random samples;recursive Bayesian filters;simulation;state transition model;state vector density;Bayes methods;Kalman filters;filtering and prediction theory;state estimation;tracking;},
  Owner                    = {iurteaga},
  Timestamp                = {2012-07-27}
}

@Article{j-Gosavi2009,
  Title                    = {{Reinforcement learning: A tutorial survey and recent advances}},
  Author                   = {Abhijit Gosavi},
  Journal                  = {INFORMS Journal on Computing},
  Year                     = {2009},
  Number                   = {2},
  Pages                    = {178--192},
  Volume                   = {21},

  Publisher                = {INFORMS}
}

@InProceedings{ip-Gruenewaelder2010,
  Title                    = {{Regret Bounds for Gaussian Process Bandit Problems}},
  Author                   = {Steffen Gr\"unew\"alder and Jean--Yves Audibert and Manfred Opper and John Shawe--Taylor},
  Booktitle                = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  Year                     = {2010},

  Address                  = {Chia Laguna Resort, Sardinia, Italy},
  Editor                   = {Yee Whye Teh and Mike Titterington},
  Month                    = {13--15 May},
  Pages                    = {273--280},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {9},

  Abstract                 = {Bandit algorithms are concerned with trading exploration with exploitation where a number of options are available but we can only learn their quality by experimenting with them. We consider the scenario in which the reward distribution for arms is modeled by a Gaussian process and there is no noise in the observed reward. Our main result is to bound the regret experienced by algorithms relative to the a posteriori optimal strategy of playing the best arm throughout based on benign assumptions about the covariance function defining the Gaussian process. We further complement these upper bounds with corresponding lower bounds for particular covariance functions demonstrating that in general there is at most a logarithmic looseness in our upper bounds.},
  Url                      = {http://proceedings.mlr.press/v9/grunewalder10a.html}
}

@Article{j-Hoffman2013,
  Title                    = {{Stochastic variational inference}},
  Author                   = {Matthew D. Hoffman and David M. Blei and Chong Wang and John William Paisley},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2013},

  Abstract                 = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions.
We develop this technique for a large class of probabilistic models and we demonstrate
it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process
topic model. Using stochastic variational inference, we analyze several large collections of
documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles
from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms
traditional variational inference, which can only handle a smaller subset. (We also show that the
Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational
inference lets us apply complex Bayesian models to massive data sets.},
  Keywords                 = {Bayesian inference, variational inference, stochastic optimization, topic models, Bayesian nonparametrics},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@Article{j-Ionides2006,
  Title                    = {{Inference for nonlinear dynamical systems}},
  Author                   = {Edward L. Ionides and C. Bret\'{o} and A. A. King},
  Journal                  = {Proceedings of the National Academy of Sciences},
  Year                     = {2006},
  Number                   = {49},
  Pages                    = {18438-18443},
  Volume                   = {103},

  Abstract                 = {Nonlinear stochastic dynamical systems are widely used to model systems across the sciences and engineering. Such models are natural to formulate and can be analyzed mathematically and numerically. However, difficulties associated with inference from time-series data about unknown parameters in these models have been a constraint on their application. We present a new method that makes maximum likelihood estimation feasible for partially-observed nonlinear stochastic dynamical systems (also known as state-space models) where this was not previously the case. The method is based on a sequence of filtering operations which are shown to converge to a maximum likelihood parameter estimate. We make use of recent advances in nonlinear filtering in the implementation of the algorithm. We apply the method to the study of cholera in Bangladesh. We construct confidence intervals, perform residual analysis, and apply other diagnostics. Our analysis, based upon a model capturing the intrinsic nonlinear dynamics of the system, reveals some effects overlooked by previous studies.},
  Owner                    = {iurteaga},
  Timestamp                = {2014-11-10}
}

@Article{j-Kalm1960,
  Title                    = {{A New Approach to Linear Filtering and Prediction Problems}},
  Author                   = {Rudolph Emil Kalman},
  Journal                  = {Transactions of the ASME--Journal of Basic Engineering},
  Year                     = {1960},
  Number                   = {Series D},
  Pages                    = {35-45},
  Volume                   = {82},

  Owner                    = {iurteaga},
  Timestamp                = {2014-11-10}
}

@InProceedings{ip-Kaufmann2012,
  Title                    = {{On Bayesian Upper Confidence Bounds for Bandit Problems}},
  Author                   = {Emilie Kaufmann and Olivier Cappe and Aurelien Garivier},
  Booktitle                = {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  Year                     = {2012},

  Address                  = {La Palma, Canary Islands},
  Editor                   = {Neil D. Lawrence and Mark Girolami},
  Month                    = {21--23 Apr},
  Pages                    = {592--600},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {22},

  Abstract                 = {Stochastic bandit problems have been analyzed from two different perspectives: a frequentist view, where the parameter is a deterministic unknown quantity, and a Bayesian approach, where the parameter is drawn from a prior distribution. We show in this paper that methods derived from this second perspective prove optimal when evaluated using the frequentist cumulated regret as a measure of performance. We give a general formulation for a class of Bayesian index policies that rely on quantiles of the posterior distribution. For binary bandits, we prove that the corresponding algorithm, termed Bayes-UCB, satisfies finite-time regret bounds that imply its asymptotic optimality. More generally, Bayes-UCB appears as an unifying framework for several variants of the UCB algorithm addressing different bandit problems (parametric multi-armed bandits, Gaussian bandits with unknown mean and variance, linear bandits). But the generality of the Bayesian approach makes it possible to address more challenging models. In particular, we show how to handle linear bandits with sparsity constraints by resorting to Gibbs sampling.},
  Owner                    = {iurteaga},
  Timestamp                = {2017.10.11}
}

@Misc{Kearns1999,
  Title                    = {{Approximate Planning in Large POMDPs via Reusable Trajectories }},

  Author                   = {Michael Kearns and Yishay Mansour and Andrew Y. Ng},
  Year                     = {1999},

  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@InCollection{ic-Kingma2015,
  Title                    = {{Variational Dropout and the Local Reparameterization Trick}},
  Author                   = {Diederik P Kingma and Tim Salimans and Max Welling},
  Booktitle                = {Advances in Neural Information Processing Systems 28},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2015},
  Editor                   = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  Pages                    = {2575--2583},

  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@InCollection{ic-Korda2013,
  Title                    = {{Thompson Sampling for 1-Dimensional Exponential Family Bandits}},
  Author                   = {Nathaniel Korda and Emilie Kaufmann and R{\'e}mi Munos},
  Booktitle                = {Advances in Neural Information Processing Systems 26},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2013},
  Editor                   = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  Pages                    = {1448--1456},

  Owner                    = {iurteaga},
  Timestamp                = {2017.05.19}
}

@InCollection{ic-Krause2011,
  Title                    = {{Contextual Gaussian Process Bandit Optimization}},
  Author                   = {Andreas Krause and Cheng S. Ong},
  Booktitle                = {Advances in Neural Information Processing Systems 24},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2011},
  Editor                   = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
  Pages                    = {2447--2455},

  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@Article{j-Lai1987,
  Title                    = {{Adaptive Treatment Allocation and the Multi-Armed Bandit Problem}},
  Author                   = {Tze Leung Lai},
  Journal                  = {The Annals of Statistics},
  Year                     = {1987},
  Number                   = {3},
  Pages                    = {1091-1114},
  Volume                   = {15},

  ISSN                     = {00905364},
  Owner                    = {iurteaga},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2017.05.10}
}

@Article{j-Lai1985,
  Title                    = {{Asymptotically Efficient Adaptive Allocation Rules}},
  Author                   = {Tze Leung Lai and Herbert Robbins},
  Journal                  = {Advances in Applied Mathematics},
  Year                     = {1985},

  Month                    = {mar},
  Number                   = {1},
  Pages                    = {4--22},
  Volume                   = {6},

  Acmid                    = {2609757},
  Address                  = {Orlando, FL, USA},
  Doi                      = {10.1016/0196-8858(85)90002-8},
  ISSN                     = {0196-8858},
  Issue_date               = {March, 1985},
  Numpages                 = {19},
  Owner                    = {iurteaga},
  Publisher                = {Academic Press, Inc.},
  Timestamp                = {2017.05.10}
}

@InProceedings{ip-Lamprier2017,
  Title                    = {{Variational Thompson Sampling for Relational Recurrent Bandits}},
  Author                   = {Sylvain Lamprier and Thibault Gisselbrecht and Patrick Gallinari},
  Booktitle                = {Machine Learning and Knowledge Discovery in Databases},
  Year                     = {2017},

  Address                  = {Cham},
  Editor                   = {Michelangelo Ceci and Jaakko Hollm{\'e}n and Ljup{\v{c}}o Todorovski and Celine Vens and Sa{\v{s}}o D{\v{z}}eroski},
  Pages                    = {405--421},
  Publisher                = {Springer International Publishing},

  Abstract                 = {In this paper, we introduce a novel non-stationary bandit setting, called relational recurrent bandit, where rewards of arms at successive time steps are interdependent. The aim is to discover temporal and structural dependencies between arms in order to maximize the cumulative collected reward. Two algorithms are proposed: the first one directly models temporal dependencies between arms, as the second one assumes the existence of hidden states of the system behind the observed rewards. For both approaches, we develop a Variational Thompson Sampling method, which approximates distributions via variational inference, and uses the estimated distributions to sample reward expectations at each iteration of the process. Experiments conducted on both synthetic and real data demonstrate the effectiveness of our approaches.},
  ISBN                     = {978-3-319-71246-8},
  Owner                    = {iurteaga},
  Timestamp                = {2018.02.06}
}

@InProceedings{ip-Langford2005,
  Title                    = {{Relating Reinforcement Learning Performance to Classification Performance}},
  Author                   = {John Langford and Bianca Zadrozny},
  Booktitle                = {Proceedings of the 22Nd International Conference on Machine Learning},
  Year                     = {2005},

  Address                  = {New York, NY, USA},
  Pages                    = {473--480},
  Publisher                = {ACM},
  Series                   = {ICML '05},

  Acmid                    = {1102411},
  Doi                      = {10.1145/1102351.1102411},
  ISBN                     = {1-59593-180-5},
  Location                 = {Bonn, Germany},
  Numpages                 = {8},
  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@InCollection{ic-Langford2008,
  Title                    = {{The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information}},
  Author                   = {John Langford and Tong Zhang},
  Booktitle                = {Advances in Neural Information Processing Systems 20},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2008},
  Editor                   = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
  Pages                    = {817--824},

  Owner                    = {iurteaga},
  Timestamp                = {2018.07.30},
  Url                      = {https://papers.nips.cc/paper/3178-the-epoch-greedy-algorithm-for-multi-armed-bandits-with-side-information}
}

@Article{j-Leeuwen2009,
  Title                    = {{Particle Filtering in Geophysical Systems}},
  Author                   = {Peter Jan van Leeuwen},
  Journal                  = {Monthly Weather Review},
  Year                     = {2009},
  Number                   = {137},
  Pages                    = {4089-4114.},
  Volume                   = {12},

  Owner                    = {iurteaga},
  Timestamp                = {2014-11-10}
}

@InProceedings{ip-Li2016,
  Title                    = {{Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks}},
  Author                   = {Chunyuan Li and Changyou Chen and David Carlson and Lawrence Carin},
  Booktitle                = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
  Year                     = {2016},
  Pages                    = {1788--1794},
  Publisher                = {AAAI Press},
  Series                   = {AAAI'16},

  Acmid                    = {3016149},
  Location                 = {Phoenix, Arizona},
  Numpages                 = {7},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@Article{j-Li2013,
  Title                    = {{Generalized Thompson Sampling for Contextual Bandits}},
  Author                   = {Lihong Li},
  Journal                  = {ArXiv e-prints},
  Year                     = {2013},

  Month                    = oct,

  Archiveprefix            = {arXiv},
  Eprint                   = {1310.7163},
  Keywords                 = {Computer Science - Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Statistics - Other Statistics, 62L05, I.2.6},
  Owner                    = {iurteaga},
  Primaryclass             = {cs.LG},
  Timestamp                = {2016.09.19}
}

@Article{j-Li2010,
  Title                    = {{A Contextual-Bandit Approach to Personalized News Article Recommendation}},
  Author                   = {Lihong Li and Wei Chu and John Langford and Robert E. Schapire},
  Journal                  = {CoRR},
  Year                     = {2010},
  Volume                   = {abs/1003.0146},

  Owner                    = {iurteaga},
  Timestamp                = {2017.04.04}
}

@Article{j-Li2015,
  Title                    = {{Resampling Methods for Particle Filtering: Classification, implementation, and strategies}},
  Author                   = {Tiancheng Li and Miodrag Boli\'{c} and Petar M. Djuri\'{c}},
  Journal                  = {Signal Processing Magazine, IEEE},
  Year                     = {2015},

  Month                    = {5},
  Number                   = {3},
  Pages                    = {70-86},
  Volume                   = {32},

  Abstract                 = {Two decades ago, with the publication, we witnessed the rebirth of particle filtering (PF) as a methodology for sequential signal processing. Since then, PF has become very popular because of its ability to process observations represented by nonlinear state-space models where the noises of the model can be non-Gaussian. This methodology has been adopted in various fields, including finance, geophysical systems, wireless communications, control, navigation and tracking, and robotics. The popularity of PF has also spurred the publication of several review articles. In this article, the state of the art of resampling methods was reviewed. The methods were classified and their properties were compared in the framework of the proposed classifications. The emphasis in the article was on the classification and qualitative descriptions of the algorithms. The intention was to provide guidelines to practitioners and researchers.},
  ISSN                     = {1053-5888},
  Keywords                 = {mobile robots;particle filtering (numerical methods);radio tracking;radionavigation;signal classification;state-space methods;telecommunication control;finance;geophysical systems;navigation;nonGaussian noise;nonlinear state-space models;particle classification;particle filtering;resampling methods;robotics;sequential signal processing;tracking;wireless communications;wireless control;Approximation algorithms;Approximation methods;Atmospheric measurements;Filtering;Particle measurements;Signal processing algorithms;Systematics},
  Owner                    = {iurteaga},
  Timestamp                = {2015-08-11}
}

@Article{j-Lipton2016,
  Title                    = {{Efficient Dialogue Policy Learning with BBQ-Networks}},
  Author                   = {Zachary C. Lipton and Xiujun Li and Jianfeng Gao and Lihong Li and Faisal Ahmed and Li Deng},
  Journal                  = {ArXiv e-prints},
  Year                     = {2016},

  Month                    = aug,

  Archiveprefix            = {arXiv},
  Eprint                   = {1608.05081},
  Keywords                 = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
  Owner                    = {iurteaga},
  Primaryclass             = {cs.LG},
  Timestamp                = {2017.11.28}
}

@InBook{ib-Liu2001,
  Title                    = {{Combined Parameter and State Estimation in Simulation-Based Filtering}},
  Author                   = {Jane Liu and Mike West},
  Chapter                  = {10},
  Editor                   = {Arnaud Doucet and Nando de Freitas and Neil Gordon},
  Pages                    = {197--223},
  Publisher                = {Springer New York},
  Year                     = {2001},

  Address                  = {New York, NY},

  Abstract                 = {Much of the recent and current interest in simulation-based methods of sequential Bayesian analysis of dynamic models has been focused on improved methods of filtering for time-varying state vectors. We now have quite effective algorithms for time-varying states, as represented throughout this volume. Variants of the auxiliary particle filtering algorithm (Pitt and Shephard 1999b), in particular, are of proven applied efficacy in quite elaborate models. However, the need for more general algorithms that deal simultaneously with both fixed model parameters and state variables is especially pressing. We simply do not have access to efficient and effective methods of treating this problem, especially in models with realistically large numbers of fixed model parameters. It is a very challenging problem.},
  Booktitle                = {Sequential Monte Carlo Methods in Practice},
  Doi                      = {10.1007/978-1-4757-3437-9_10},
  ISBN                     = {978-1-4757-3437-9},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.14}
}

@Book{b-Liu2001,
  Title                    = {{Monte Carlo Strategies in Scientific Computing}},
  Author                   = {Jun S. Liu},
  Publisher                = {Springer},
  Year                     = {2001},
  Series                   = {Springer Series in Statistics},

  Abstract                 = {This book provides a self-contained and up-to-date treatment of the Monte Carlo method and develops a common framework under which various Monte Carlo techniques can be "standardized" and compared. Given the interdisciplinary nature of the topics and a moderate prerequisite for the reader, this book should be of interest to a broad audience of quantitative researchers such as computational biologists, computer scientists, econometricians, engineers, probabilists, and statisticians. It can also be used as the textbook for a graduate-level course on Monte Carlo methods. Many problems discussed in the alter chapters can be potential thesis topics for masters or Ph.D. students in statistics or computer science departments.},
  Keywords                 = {Monte Carlo method; Sequential Monte-Carlo; Metropolis Algorithm; Gibbs Sampler; Conditional Sampling; Population-Based Monte Carlo Methods},
  Owner                    = {iurteaga},
  Pages                    = {346},
  Timestamp                = {2012-08-20}
}

@InProceedings{ip-Maillard2011,
  Title                    = {{Finite-Time Analysis of Multi-armed Bandits Problems with Kullback-Leibler Divergences}},
  Author                   = {Odalric-Ambrym Maillard and R{\'e}mi Munos and Gilles Stoltz},
  Booktitle                = {Conference On Learning Theory},
  Year                     = {2011},

  Abstract                 = {We consider a Kullback-Leibler-based algorithm for the stochastic multi-armed bandit problem in the case of distributions with finite supports (not necessarily known beforehand), whose asymptotic regret matches the lower bound of Burnetas and Katehakis (1996). Our contribution is to provide a finite-time analysis of this algorithm; we get bounds whose main terms are smaller than the ones of previously known algorithms with finite-time analyses (like UCB-type algorithms)},
  Owner                    = {iurteaga},
  Timestamp                = {2017.05.19}
}

@Article{j-Martino2017,
  Title                    = {{Effective sample size for importance sampling based on discrepancy measures}},
  Author                   = {Luca Martino and V\'{i}ctor Elvira and Francisco Louzada},
  Journal                  = {Signal Processing },
  Year                     = {2017},
  Pages                    = {386 - 401},
  Volume                   = {131},

  Abstract                 = {Abstract The Effective Sample Size (ESS) is an important measure of efficiency of Monte Carlo methods such as Markov Chain Monte Carlo (MCMC) and Importance Sampling (IS) techniques. In the \{IS\} context, an approximation \{ESS\} ^ of the theoretical \{ESS\} definition is widely applied, involving the inverse of the sum of the squares of the normalized importance weights. This formula, \{ESS\} ^ , has become an essential piece within Sequential Monte Carlo (SMC) methods, to assess the convenience of a resampling step. From another perspective, the expression \{ESS\} ^ is related to the Euclidean distance between the probability mass described by the normalized weights and the discrete uniform probability mass function (pmf). In this work, we derive other possible \{ESS\} functions based on different discrepancy measures between these two pmfs. Several examples are provided involving, for instance, the geometric mean of the weights, the discrete entropy (including the perplexity measure, already proposed in literature) and the Gini coefficient among others. We list five theoretical requirements which a generic \{ESS\} function should satisfy, allowing us to classify different \{ESS\} measures. We also compare the most promising ones by means of numerical simulations. },
  ISSN                     = {0165-1684},
  Keywords                 = {Effective Sample Size},
  Owner                    = {iurteaga},
  Timestamp                = {2017.07.09}
}

@Book{b-McLachlan2004,
  Title                    = {{Finite Mixture Models}},
  Author                   = {Geoffrey McLachlan and David Peel},
  Publisher                = {John Wiley \& Sons, 2004},
  Year                     = {2004},
  Series                   = {Wiley Series in Probability and Statistics},

  ISBN                     = {9780471654063},
  Owner                    = {iurteaga},
  Timestamp                = {2017.05.12}
}

@Article{j-Neal2000,
  Title                    = {{Markov Chain Sampling Methods for Dirichlet Process Mixture Models}},
  Author                   = {Radford M. Neal},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2000},
  Number                   = {2},
  Pages                    = {249--265},
  Volume                   = {9},

  Abstract                 = {This article reviews Markov chain methods for sampling from the posterior distribution of a Dirichlet process mixture model and presents two new classes of methods. One new approach is to make Metropolis-Hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of Gibbs sampling. The other new approach extends Gibbs sampling for these indicators by using a set of auxiliary parameters. These methods are simple to implement and are more efficient than previous ways of handling general Dirichlet process mixture models with non-conjugate priors.},
  ISSN                     = {10618600},
  Owner                    = {iurteaga},
  Publisher                = {[American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
  Timestamp                = {2018.05.16}
}

@Article{j-Olsson2006,
  Title                    = {{Sequential Monte Carlo smoothing with application to parameter estimation in non-linear state space models}},
  Author                   = {{Olsson}, J. and {Capp{\'e}}, O. and {Douc}, R. and {Moulines}, E.},
  Journal                  = {ArXiv Mathematics e-prints},
  Year                     = {2006},

  Month                    = {Sep},

  Eprint                   = {math/0609514},
  Keywords                 = {Mathematics - Statistics},
  Owner                    = {iurteaga},
  Timestamp                = {2017.07.09}
}

@Article{j-Olsson2014,
  Title                    = {{Efficient particle-based online smoothing in general hidden Markov models: the PaRIS algorithm}},
  Author                   = {{Olsson}, J. and {Westerborn}, J.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2014},

  Month                    = {Dec},

  Archiveprefix            = {arXiv},
  Eprint                   = {1412.7550},
  Keywords                 = {Statistics - Computation, 62M09 (Primary) 62F12 (Secondary)},
  Owner                    = {iurteaga},
  Primaryclass             = {stat.CO},
  Timestamp                = {2017.07.09}
}

@Article{j-Ortega2010,
  Title                    = {{A Minimum Relative Entropy Principle for Learning and Acting}},
  Author                   = {Pedro A. Ortega and Daniel A. Braun},
  Journal                  = {Journal of Artificial Intelligence Research},
  Year                     = {2010},

  Month                    = may,
  Number                   = {1},
  Pages                    = {475--511},
  Volume                   = {38},

  Acmid                    = {1892223},
  Address                  = {USA},
  ISSN                     = {1076-9757},
  Issue_date               = {May 2010},
  Numpages                 = {37},
  Owner                    = {iurteaga},
  Publisher                = {AI Access Foundation},
  Timestamp                = {2016.09.19}
}

@InCollection{ic-Osband2016,
  Title                    = {{Deep Exploration via Bootstrapped DQN}},
  Author                   = {Ian Osband and Charles Blundell and Alexander Pritzel and Benjamin Van Roy},
  Booktitle                = {Advances in Neural Information Processing Systems 29},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2016},
  Editor                   = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  Pages                    = {4026--4034},

  Owner                    = {iurteaga},
  Timestamp                = {2017.11.28}
}

@InCollection{ic-Ouyang2017,
  Title                    = {{Learning Unknown Markov Decision Processes: A Thompson Sampling Approach}},
  Author                   = {Yi Ouyang and Mukul Gagrani and Ashutosh Nayyar and Rahul Jain},
  Booktitle                = {Advances in Neural Information Processing Systems 30},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2017},
  Editor                   = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  Pages                    = {1333--1342},

  Owner                    = {iurteaga},
  Timestamp                = {2018.07.30},
  Url                      = {http://papers.nips.cc/paper/6732-learning-unknown-markov-decision-processes-a-thompson-sampling-approach.pdf}
}

@Article{j-Pearl2009,
  Title                    = {{Causal inference in statistics: An overview}},
  Author                   = {Judea Pearl},
  Journal                  = {Statist. Surv.},
  Year                     = {2009},
  Pages                    = {96--146},
  Volume                   = {3},

  Doi                      = {10.1214/09-SS057},
  Fjournal                 = {Statistics Surveys},
  Owner                    = {iurteaga},
  Publisher                = {The American Statistical Association, the Bernoulli Society, the Institute of Mathematical Statistics, and the Statistical Society of Canada},
  Timestamp                = {2016.09.14}
}

@Article{j-Pivovarov2015,
  Title                    = {{Learning probabilistic phenotypes from heterogeneous \{EHR\} data}},
  Author                   = {Rimma Pivovarov and Adler J. Perotte and Edouard Grave and John Angiolillo and Chris H. Wiggins and Noémie Elhadad},
  Journal                  = {Journal of Biomedical Informatics},
  Year                     = {2015},
  Pages                    = {156 - 165},
  Volume                   = {58},

  Abstract                 = {Abstract We present the Unsupervised Phenome Model (UPhenome), a probabilistic graphical model for large-scale discovery of computational models of disease, or phenotypes. We tackle this challenge through the joint modeling of a large set of diseases and a large set of clinical observations. The observations are drawn directly from heterogeneous patient record data (notes, laboratory tests, medications, and diagnosis codes), and the diseases are modeled in an unsupervised fashion. We apply \{UPhenome\} to two qualitatively different mixtures of patients and diseases: records of extremely sick patients in the intensive care unit with constant monitoring, and records of outpatients regularly followed by care providers over multiple years. We demonstrate that the \{UPhenome\} model can learn from these different care settings, without any additional adaptation. Our experiments show that (i) the learned phenotypes combine the heterogeneous data types more coherently than baseline LDA-based phenotypes; (ii) they each represent single diseases rather than a mix of diseases more often than the baseline ones; and (iii) when applied to unseen patient records, they are correlated with the patients’ ground-truth disorders. Code for training, inference, and quantitative evaluation is made available to the research community. },
  Doi                      = {http://dx.doi.org/10.1016/j.jbi.2015.10.001},
  ISSN                     = {1532-0464},
  Keywords                 = {Probabilistic modeling, Computational disease models, Phenotyping, Clinical phenotype modeling, Medical information systems, Electronic health record },
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@Book{b-Rasmussen2005,
  Title                    = {{Gaussian Processes for Machine Learning}},
  Author                   = {Carl Edward Rasmussen and Christopher K. I. Williams},
  Publisher                = {The MIT Press},
  Year                     = {2005},

  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@InProceedings{ip-Riquelme2018,
  Title                    = {{Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling}},
  Author                   = {Carlos Riquelme and George Tucker and Jasper Snoek},
  Booktitle                = {International Conference on Learning Representations},
  Year                     = {2018},

  Owner                    = {iurteaga},
  Timestamp                = {2018.05.15}
}

@Book{b-Ristic2004,
  Title                    = {{Beyond the Kalman Filter: Particle Filters for Tracking Applications}},
  Author                   = {Branko Ristic and Sanjeev Arulampalam and Neil Gordon},
  Editor                   = {Artech Print},
  Publisher                = {Artech House},
  Year                     = {2004},

  ISBN                     = {9781580538510},
  Owner                    = {iurteaga},
  Timestamp                = {2014-11-10}
}

@Article{j-Robbins1956,
  Title                    = {{A sequential decision procedure with a finite memory}},
  Author                   = {Herbert Robbins},
  Journal                  = {Proceedings of the National Academy of Science},
  Year                     = {1956},
  Number                   = {42},
  Pages                    = {920 - 923},

  Owner                    = {iurteaga},
  Timestamp                = {2017.05.10}
}

@Article{j-Robbins1952,
  Title                    = {{Some aspects of the sequential design of experiments}},
  Author                   = {Herbert Robbins},
  Journal                  = {Bulletin of the American Mathematical Society},
  Year                     = {1952},
  Number                   = {58},
  Pages                    = {527-535},

  Owner                    = {iurteaga},
  Timestamp                = {2017.05.10}
}

@Article{j-Rusmevichientong2010,
  Title                    = {Linearly Parameterized Bandits},
  Author                   = {Paat Rusmevichientong and John N. Tsitsiklis},
  Journal                  = {Mathematics of Operations Research},
  Year                     = {2010},

  Month                    = may,
  Number                   = {2},
  Pages                    = {395--411},
  Volume                   = {35},

  Acmid                    = {1836129},
  Address                  = {Institute for Operations Research and the Management Sciences (INFORMS), Linthicum, Maryland, USA},
  Doi                      = {10.1287/moor.1100.0446},
  ISSN                     = {0364-765X},
  Issue_date               = {May 2010},
  Keywords                 = {adaptive control, multi-armed bandit, parametric model},
  Numpages                 = {17},
  Owner                    = {iurteaga},
  Publisher                = {INFORMS},
  Timestamp                = {2018.07.30}
}

@Article{j-Russo2016,
  Title                    = {{An information-theoretic analysis of Thompson sampling}},
  Author                   = {Daniel Russo and Benjamin Van Roy},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2016},
  Number                   = {1},
  Pages                    = {2442--2471},
  Volume                   = {17},

  Keywords                 = {Online optimization; Multi-armed bandits; Thompson sampling; Information theory; Regret bounds;},
  Owner                    = {iurteaga},
  Publisher                = {JMLR. org},
  Timestamp                = {2017.08.02}
}

@Article{j-Russo2014,
  Title                    = {{Learning to optimize via posterior sampling}},
  Author                   = {Daniel Russo and Benjamin Van Roy},
  Journal                  = {Mathematics of Operations Research},
  Year                     = {2014},
  Number                   = {4},
  Pages                    = {1221--1243},
  Volume                   = {39},

  Keywords                 = {Online optimization; Multi-armed bandits; Thompson sampling; theoretical bounds;},
  Owner                    = {iurteaga},
  Timestamp                = {2017.08.02}
}

@Article{j-Russo2018,
  Title                    = {{A Tutorial on Thompson Sampling}},
  Author                   = {Daniel J. Russo and Benjamin Van Roy and Abbas Kazerouni and Ian Osband and Zheng Wen},
  Journal                  = {{Foundations and Trends\textsuperscript{\textregistered} in Machine Learning}},
  Year                     = {2018},
  Number                   = {1},
  Pages                    = {1-96},
  Volume                   = {11},

  Doi                      = {10.1561/2200000070},
  ISSN                     = {1935-8237},
  Owner                    = {iurteaga},
  Timestamp                = {2018.07.30},
  Url                      = {http://dx.doi.org/10.1561/2200000070}
}

@Article{j-Scott2015,
  Title                    = {{Multi-armed bandit experiments in the online service economy}},
  Author                   = {Steven L. Scott},
  Journal                  = {Applied Stochastic Models in Business and Industry},
  Year                     = {2015},
  Note                     = {Special issue on actual impact and future perspectives on stochastic modelling in business and industry},
  Pages                    = {37--49},
  Volume                   = {31},

  Abstract                 = {The modern service economy is substantively different from the agricultural and manufacturing economies that preceded it. In particular, the cost of experimenting is dominated by opportunity cost rather than the cost of obtaining experimental units. The different economics require a new class of experiments, in which stochastic models play an important role. This article briefly summarizes mulit-armed bandit experiments, where the experimental design is modified as the experiment progresses to make the experiment as inexpensive as possible.},
  Owner                    = {iurteaga},
  Timestamp                = {2017.04.04}
}

@Article{j-Scott2010,
  Title                    = {{A modern Bayesian look at the multi-armed bandit}},
  Author                   = {Steven L. Scott},
  Journal                  = {Applied Stochastic Models in Business and Industry},
  Year                     = {2010},
  Number                   = {6},
  Pages                    = {639--658},
  Volume                   = {26},

  Doi                      = {10.1002/asmb.874},
  ISSN                     = {1526-4025},
  Keywords                 = {probability matching, exploration vs exploitation, sequential design, Bayesian adaptive design},
  Owner                    = {iurteaga},
  Publisher                = {John Wiley \& Sons, Ltd.},
  Timestamp                = {2016.09.19}
}

@Article{j-Shahriari2016,
  Title                    = {{Taking the Human Out of the Loop: A Review of Bayesian Optimization}},
  Author                   = {Bobak Shahriari and Kevin Swersky and Ziyu Wang and Ryan P. Adams and Nando de Freitas},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {148-175},
  Volume                   = {104},

  Abstract                 = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  Doi                      = {10.1109/JPROC.2015.2494218},
  ISSN                     = {0018-9219},
  Keywords                 = {Bayes methods;Big Data;optimisation;storage allocation;Bayesian optimization;Big data application;human productivity;large-scale heterogeneous computing;massive complex software system;product quality;storage architecture;Bayes methods;Big data;Decision making;Design of experiments;Genomes;Linear programming;Optimization;Statistical analysis;Decision making;decision making;design of experiments;genomic medicine;optimization;response surface methodology;statistical learning},
  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@InCollection{ic-Snelson2006,
  Title                    = {{Sparse Gaussian Processes using Pseudo-inputs}},
  Author                   = {Edward Snelson and Zoubin Ghahramani},
  Booktitle                = {Advances in Neural Information Processing Systems 18},
  Publisher                = {MIT Press},
  Year                     = {2006},
  Editor                   = {Y. Weiss and B. Sch\"{o}lkopf and J. C. Platt},
  Pages                    = {1257--1264},

  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@InProceedings{ip-Srinivas2010,
  Title                    = {{Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design}},
  Author                   = {Niranjan Srinivas and Andreas Krause and Sham Kakade and Matthias Seeger},
  Booktitle                = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
  Year                     = {2010},

  Address                  = {USA},
  Pages                    = {1015--1022},
  Publisher                = {Omnipress},
  Series                   = {ICML'10},

  Acmid                    = {3104451},
  ISBN                     = {978-1-60558-907-7},
  Location                 = {Haifa, Israel},
  Numpages                 = {8},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@Article{j-Strehl2010,
  Title                    = {{Learning from Logged Implicit Exploration Data}},
  Author                   = {Alexander L. Strehl and John Langford and Sham M. Kakade},
  Journal                  = {CoRR},
  Year                     = {2010},
  Volume                   = {abs/1003.0120},

  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@InProceedings{ip-Strehl2010,
  Title                    = {{Learning from Logged Implicit Exploration Data}},
  Author                   = {Alexander L. Strehl and John Langford and Lihong Li and Sham Kakade},
  Booktitle                = {Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia,
 Canada.},
  Year                     = {2010},
  Pages                    = {2217--2225},

  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@Book{b-Sutton1998,
  Title                    = {{Reinforcement Learning: An Introduction}},
  Author                   = {Richard S. Sutton and Andrew G. Barto},
  Publisher                = {MIT Press: Cambridge, MA},
  Year                     = {1998},

  Owner                    = {iurteaga},
  Timestamp                = {2017.05.10}
}

@Article{j-Teh2010,
  Title                    = {{Hierarchical Bayesian nonparametric models with applications}},
  Author                   = {Yee Whye Teh and Michael I. Jordan},
  Journal                  = {Bayesian nonparametrics},
  Year                     = {2010},
  Volume                   = {1},

  Keywords                 = {Hierarchical Bayesian; Hierarchical Nonparametric; Hierarchical Dirichlet Process; Pitman-Yor process; Indian Buffet; Beta process;},
  Owner                    = {iurteaga},
  Publisher                = {Camb. Ser. Stat. Probab. Math},
  Timestamp                = {2017.07.07}
}

@Article{j-Teh2006,
  Title                    = {{Hierarchical Dirichlet Processes}},
  Author                   = {Yee Whye Teh and Michael I Jordan and Matthew J Beal and David M Blei},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2006},
  Number                   = {476},
  Pages                    = {1566-1581},
  Volume                   = {101},

  Abstract                 = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
  Doi                      = {10.1198/016214506000000302},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@InProceedings{ip-Teh2008,
  Title                    = {{Collapsed Variational Inference for HDP}},
  Author                   = {Yee Whye Teh and Kenichi Kurihara and Max Welling},
  Booktitle                = {Advances in Neural Information Processing Systems},
  Year                     = {2008},
  Volume                   = {20},

  Keywords                 = {HDP, variational},
  Owner                    = {iurteaga},
  Timestamp                = {2017.04.20}
}

@InProceedings{ip-Teh2007,
  Title                    = {{A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation}},
  Author                   = {Yee Whye Teh and David Newman and Max Welling},
  Booktitle                = {Advances in Neural Information Processing Systems},
  Year                     = {2007},
  Volume                   = {19},

  Owner                    = {iurteaga},
  Timestamp                = {2017.04.20}
}

@Article{j-Thompson1935,
  Title                    = {{On the Theory of Apportionment}},
  Author                   = {William R. Thompson},
  Journal                  = {American Journal of Mathematics},
  Year                     = {1935},
  Number                   = {2},
  Pages                    = {450-456},
  Volume                   = {57},

  ISSN                     = {00029327, 10806377},
  Owner                    = {iurteaga},
  Publisher                = {Johns Hopkins University Press},
  Timestamp                = {2016.09.20}
}

@Article{j-Thompson1933,
  Title                    = {{On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples}},
  Author                   = {William R. Thompson},
  Journal                  = {Biometrika},
  Year                     = {1933},
  Number                   = {3/4},
  Pages                    = {285-294},
  Volume                   = {25},

  ISSN                     = {00063444},
  Owner                    = {iurteaga},
  Publisher                = {[Oxford University Press, Biometrika Trust]},
  Timestamp                = {2016.09.19}
}

@InProceedings{ip-Titsias2009,
  Title                    = {{Variational Learning of Inducing Variables in Sparse Gaussian Processes}},
  Author                   = {Michalis Titsias},
  Booktitle                = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  Year                     = {2009},

  Address                  = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  Editor                   = {David van Dyk and Max Welling},
  Month                    = {16--18 Apr},
  Pages                    = {567--574},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {5},

  Abstract                 = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
  File                     = {titsias09a.pdf:http\://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf:PDF},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@InProceedings{ip-Urteaga2018,
  Title                    = {{Variational inference for the multi-armed contextual bandit}},
  Author                   = {I{\~{n}}igo Urteaga and Chris Wiggins},
  Booktitle                = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  Year                     = {2018},

  Address                  = {Playa Blanca, Lanzarote, Canary Islands},
  Editor                   = {Amos Storkey and Fernando Perez-Cruz},
  Month                    = {09--11 Apr},
  Pages                    = {698--706},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {84},

  Abstract                 = {In many biomedical, science, and engineering problems, one must sequentially decide which action to take next so as to maximize rewards. One general class of algorithms for optimizing interactions with the world, while simultaneously learning how the world operates, is the multi-armed bandit setting and, in particular, the contextual bandit case. In this setting, for each executed action, one observes rewards that are dependent on a given context, available at each interaction with the world. The Thompson sampling algorithm has recently been shown to enjoy provable optimality properties for this set of problems, and to perform well in real-world settings. It facilitates generative and interpretable modeling of the problem at hand. Nevertheless, the design and complexity of the model limit its application, since one must both sample from the distributions modeled and calculate their expected rewards. We here show how these limitations can be overcome using variational inference to approximate complex models, applying to the reinforcement learning case advances developed for the inference case in the machine learning community over the past two decades. We consider contextual multi-armed bandit applications where the true reward distribution is unknown and complex, which we approximate with a mixture model whose parameters are inferred via variational inference. We show how the proposed variational Thompson sampling approach is accurate in approximating the true distribution, and attains reduced regrets even with complex reward distributions. The proposed algorithm is valuable for practical scenarios where restrictive modeling assumptions are undesirable.},
  File                     = {urteaga18a.pdf:http\://proceedings.mlr.press/v84/urteaga18a/urteaga18a.pdf:PDF},
  Owner                    = {iurteaga},
  Timestamp                = {2018.02.06}
}

@Article{j-Urteaga2018,
  Title                    = {{(Sequential) Importance Sampling Bandits}},
  Author                   = {I{\~{n}}igo Urteaga and Chris Wiggins},
  Journal                  = {ArXiv e-prints},
  Year                     = {2018},

  Month                    = sep,

  Archiveprefix            = {arXiv},
  Eprint                   = {1709.03162},
  Keywords                 = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, I.2.6},
  Owner                    = {iurteaga},
  Primaryclass             = {stat.ML},
  Timestamp                = {2018.08.08}
}

@Article{j-Urteaga2018a,
  Title                    = {{Nonparametric Gaussian mixture models for the multi-armed contextual bandit}},
  Author                   = {I{\~{n}}igo Urteaga and Chris Wiggins},
  Journal                  = {ArXiv e-prints},
  Year                     = {2018},

  Month                    = sep,

  Archiveprefix            = {arXiv},
  Eprint                   = {1709.03162},
  Keywords                 = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, I.2.6},
  Owner                    = {iurteaga},
  Primaryclass             = {stat.ML},
  Timestamp                = {2018.08.08}
}

@Article{j-Urteaga2017,
  Title                    = {{Bayesian bandits: balancing the exploration-exploitation tradeoff via double sampling}},
  Author                   = {I{\~{n}}igo Urteaga and Chris Wiggins},
  Journal                  = {ArXiv e-prints},
  Year                     = {2017},

  Month                    = sep,

  Archiveprefix            = {arXiv},
  Eprint                   = {1709.03162},
  Keywords                 = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, I.2.6},
  Owner                    = {iurteaga},
  Primaryclass             = {stat.ML},
  Timestamp                = {2018.08.08}
}

@Article{j-Wang2012,
  Title                    = {{Continuous time dynamic topic models}},
  Author                   = {Chong Wang and Dvid Blei and Dvaid Heckerman},
  Journal                  = {arXiv preprint arXiv:1206.3298},
  Year                     = {2012},

  Abstract                 = {In this paper, we develop the continuous time dynamic topic model (cDTM). The cDTM is a dynamic topic model that uses Brownian motion to model the latent topics through a sequential collection of documents, where a "topic" is a pattern of word use that we expect to evolve over the course of the collection. We derive an efficient variational approximate inference algorithm that takes advantage of the sparsity of observations in text, a property that lets us easily handle many time points. In contrast to the cDTM, the original discrete-time dynamic topic model (dDTM) requires that time be discretized. Moreover, the complexity of variational inference for the dDTM grows quickly as time granularity increases, a drawback which limits fine-grained discretization. We demonstrate the cDTM on two news corpora, reporting both predictive perplexity and the novel task of time stamp prediction.},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@InProceedings{ip-Wang2009,
  Title                    = {{Markov Topic Models}},
  Author                   = {Chong Wang and Bo Thiesson and Chris Meek and David Blei},
  Booktitle                = {D. van Dyk and M. Welling (Eds.), Proceedings of The Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS) 2009, JMLR: W\&CP 5},
  Year                     = {2009},
  Month                    = {April},
  Pages                    = {583-590},
  Publisher                = {Journal of Machine Learning Research},

  Abstract                 = {We develop Markov topic models (MTMs), a novel family of generative probabilistic models that can learn topics simultaneously from multiple corpora, such as papers from different conferences. We apply Gaussian (Markov) random fields to model the correlations of different corpora. MTMs capture both the internal topic structure within each corpus and the relationships between topics across the corpora. We derive an efficient estimation procedure with variational expectation-maximization. We study the performance of our models on a corpus of abstracts from six different computer science conferences. Our analysis reveals qualitative discoveries that are not possible with traditional topic models, and improved quantitative performance over the state of the art.},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@InProceedings{ip-Wang2006,
  Title                    = {{Topics over Time: A non-Markov Continuous-time Model of Topical Trends}},
  Author                   = {Xuerui Wang and Andrew McCallum},
  Booktitle                = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  Year                     = {2006},

  Address                  = {New York, NY, USA},
  Pages                    = {424--433},
  Publisher                = {ACM},
  Series                   = {KDD '06},

  Acmid                    = {1150450},
  Doi                      = {10.1145/1150402.1150450},
  ISBN                     = {1-59593-339-5},
  Keywords                 = {graphical models, temporal analysis, topic modeling},
  Location                 = {Philadelphia, PA, USA},
  Numpages                 = {10},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@Book{b-Doucet2001,
  Title                    = {{Sequential Monte Carlo Methods in Practice}},
  Editor                   = {Arnaud Doucet and Nando De Freitas and Neil Gordon},
  Publisher                = {Springer},
  Year                     = {2001},

  Abstract                 = {{I: Introduction. An introduction to sequential Monte Carlo methods / Arnaud Doucet, Nando de Freitas, and Deil Gordon -- II: Theoretical issues. Particle filters-a theoretical perspective / Dan Crisan -- Interacting particle filtering with discrete observations / Pierre Del Moral and Jean Jacod / III: Strategies for improving sequential Monte Carlo methods. Sequential Monte Carlo methods for optimal filtering / Christophe Andrieu, Arnaud Doucet, and Elena Punskaya -- Deterministic and stochastic particle filters in state-space models / Erik Bolviken and Geir Storvik -- RESAMPLE-MOVE filtering with cross-model jumps / Carlo Berzuini and Walter Gilks -- Improvement strategies for Monte Carlo particle filters / Simon Godsill and Tim Clapp -- Approximating and maximising the likelihood for a general state-space model / Markus Hurzeler and Hans R. Kunsch -- Monte Carlo smoothing and self-organising state-space model / Genshiro Kitagawa and Seisho Sato -- Combined parameter and state estimation in simulation-based filtering / Jane Liu and Mike West -- A theoretical framework for sequential importance sampling with reasampling / Jun S. Liu, Rong Chen, and Tanya Logvinenko -- Improving regularised particle filters / Christian Musso, Nadia Oudjane, and Francois Le Gland -- Auxiliary variable based particle filters / Michael K. Pitt and Neil Shephard -- Improved particle filters and smoothing / Photis Stavropoulos and D.M. Titterington -- IV: Applications. Posterior Cramer-Rao bounds for sequential estimation / Niclas Bergman -- Statistical models of visual shape and motion / Andrew Blake, Michael Isard, and John MacCormick -- Sequential Monte Carlo methods for neural networks / N de Freitas...[et al.] -- Sequential estimation of signals under model uncertainty / Petar M. Djuric -- Particle filters for mobile robot localization / Dieter Fox...[et al.] -- Self-organizing time series model / Tomoyuki Higuchi -- Sampling in factored dynamic systems / Daphne Koller and Uri Lerner -- In-situ ellipsometry solutions using sequential Monte Carlo / Alan D. Marrs -- Manoeuvring target tracking using a multiple-model bootstrap filter / Shaun McGinnity and George W. Irwin -- Rao-Blackwellised particle filtering for dynamic Bayesian networks / Kevin Murphy and Stuart Russell -- Particles and mixtures for tracking and guidance / David Salmond and Neil Gordon -- Monte Carlo techniques for automated target recognition / Anuj Srivastava...[et al.].}},
  Keywords                 = {Monte Carlo Methods, filtering, Particle filtering, Sequential Monte Carlo;},
  Owner                    = {iurteaga},
  Timestamp                = {2014-06-03}
}

@Article{j-Cherkassky2013,
  author        = {Michael Cherkassky and Luke Bornn},
  title         = {{Sequential Monte Carlo Bandits}},
  journal       = {ArXiv e-prints},
  year          = {2013},
  month         = oct,
  archiveprefix = {arXiv},
  eprint        = {1310.1404},
  keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning, Statistics - Methodology},
  primaryclass  = {stat.ML},
}

@Comment{jabref-meta: databaseType:bibtex;}
